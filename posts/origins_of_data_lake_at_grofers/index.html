<!doctype html><html lang=en class=direction-ltr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Origins of Data Lake at Grofers -</title><meta name=keywords content="data lake,apache hudi"><meta name=description content="Originally published over lambda.grofers.com
The Beginning As Data Engineers at the biggest online grocery company in India, one of our major challenges is to democratize data across the organization. But, when we began evaluating how well we were doing on this goal, we realized that frequent errors in our data pipelines had caused our business teams to lose confidence in data. As a result, they were never sure which data source is correct and apt to use for their analyses and had to consult the data platform team at every step."><meta name=author content="Me"><link rel=canonical href=https://akshay2agarwal.github.io/posts/origins_of_data_lake_at_grofers/><link href=https://akshay2agarwal.github.io/assets/css/stylesheet.min.a7be9e3b28868ad9d1331f00b5e9c207efbfe37aea15c32d101d76f2901bb918.css integrity="sha256-p76eOyiGitnRMx8AtenCB++/43rqFcMtEB128pAbuRg=" rel="preload stylesheet" as=style><link rel=apple-touch-icon href=https://akshay2agarwal.github.io/apple-touch-icon.png><link rel=icon href=https://akshay2agarwal.github.io/favicon.ico><meta name=generator content="Hugo 0.76.5"><meta property="og:title" content="Origins of Data Lake at Grofers"><meta property="og:description" content="Originally published over lambda.grofers.com
The Beginning As Data Engineers at the biggest online grocery company in India, one of our major challenges is to democratize data across the organization. But, when we began evaluating how well we were doing on this goal, we realized that frequent errors in our data pipelines had caused our business teams to lose confidence in data. As a result, they were never sure which data source is correct and apt to use for their analyses and had to consult the data platform team at every step."><meta property="og:type" content="article"><meta property="og:url" content="https://akshay2agarwal.github.io/posts/origins_of_data_lake_at_grofers/"><meta property="og:image" content="https://cdn-images-1.medium.com/max/6000/1*e2wHxBZ0lfZCcp5EXg__Iw.jpeg"><meta property="article:published_time" content="2020-09-15T11:30:03+00:00"><meta property="article:modified_time" content="2020-09-15T11:30:03+00:00"><meta property="og:site_name" content="Akshay Agarwal"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cdn-images-1.medium.com/max/6000/1*e2wHxBZ0lfZCcp5EXg__Iw.jpeg"><meta name=twitter:title content="Origins of Data Lake at Grofers"><meta name=twitter:description content="Originally published over lambda.grofers.com
The Beginning As Data Engineers at the biggest online grocery company in India, one of our major challenges is to democratize data across the organization. But, when we began evaluating how well we were doing on this goal, we realized that frequent errors in our data pipelines had caused our business teams to lose confidence in data. As a result, they were never sure which data source is correct and apt to use for their analyses and had to consult the data platform team at every step."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Origins of Data Lake at Grofers","name":"Origins of Data Lake at Grofers","description":"Originally published over lambda.grofers.com\nThe Beginning As Data Engineers at the biggest online grocery company in India, one of our major challenges is to democratize data …","keywords":["data lake","apache hudi"],"articleBody":"Originally published over lambda.grofers.com\nThe Beginning As Data Engineers at the biggest online grocery company in India, one of our major challenges is to democratize data across the organization. But, when we began evaluating how well we were doing on this goal, we realized that frequent errors in our data pipelines had caused our business teams to lose confidence in data. As a result, they were never sure which data source is correct and apt to use for their analyses and had to consult the data platform team at every step. This wasn’t where we aspired to be when we provided the tools for making correct data-based decisions as independently as possible without getting slowed down.\nAs modern data platforms gather data from many disparate, disconnected, and diverse systems they are prone to data collection issues like duplicate records, missed updates, etc. To resolve these problems we conducted a thorough study of our data platform and realized that the architectural debts accumulated over time caused most cases of incorrect data. All major functions of our data platform — extraction, transformation, and storage had issues that led to the stated quality concerns with our data platform.\nLet’s first list down these issues:\n1. Lack of separation between raw and processed data When we started our data journey 5 years ago, we did not have the foresight to separate our source tables from our derived tables. Thus the application tables got dumped into the same warehouse along with schema. While this was okay when we only had 20 tables, it became a huge problem when that number grew to cross the 1000 mark.\nNot only were source tables placed right next to the data marts built on top of these tables, oftentimes we would make modifications to the source tables themselves. As a result, data consumers were often unsure about the meaning of data contained in different tables and found it hard to determine which table or column to use as the source of truth.\nFrom an engineering perspective, it had started becoming difficult for us to trace lineages of data points during troubleshooting, making our MTTR high and causing regular disruption for end users very frequently. This lack of separation also took a toll on our infrastructure bills as we were keeping raw tables along with marts in the same storage.\n2. Limitations of batched SQL based loading In the beginning, our source tables were replicated using scheduled, batched, SQL based pulls from our production databases.\nThese loading jobs had certain problems which were inherent to batched jobs:\n  These operations needed to rely on one fixed column, e.g. the primary key, “created_at” or “updated_at” fields, which could be used as a marker to keep track of the number of rows that had already been replicated so that the subsequent job could begin where the previous one had left off. However, this failed to replicate the changes that would not be visible through the tracking column. For example, imagine using the “updated_at” column as your tracker and you DELETE a row in your source table.\n  You had to be precise when specifying the boundary conditions like the next marker time from which data has to be queried. Oftentimes, analysts failed to consider this while defining their jobs and you would see duplicate or missing entries around the edges of your conditions.\n  We had some business-critical tables which get a high frequency of updates. So much so that the tables mutate multiple times while the replication is still in progress. This would either cause the jobs to fail because of row conflicts or succeed but produce data that differs from the source since redshift does not have native merge command. To circumvent this problem, we had to add more tools to the platform, which in turn became fragmented, increasing the complexity and adding more points of failure.\n  3. Components that would not scale To allow data analysts to program their replication operations without any help from the data engineering team, we introduced a collection of visual drag-drop tools. While this allowed us to scale up our data operations in the beginning, it quickly turned into an unmanageable nightmare. A couple of years down the line, finding the job that populated X table had gone from difficult to impossible.\nAnother drawback of some of the tools that we had been using to run the bulk of our jobs had very little to no ability to raise alerts when something went wrong. This led to a situation where we were often unaware of any problems or inconsistencies in our data and only found out when some data analyst raised a concern. Add to that the fact that these tools started taking more time for execution with increasing scale, we had a very expensive problem on our hands.\n4. No support for real-time pipelines As the organization grew, we had started seeing more and more real-time use cases come up and there was no way to extend the existing data platform to cater to these scenarios.\nWith so many issues plaguing our warehouse, we realized that we had come to the end of the road with the first generation of our data warehouse. It was at this point that we decided to take a step back, think of what we needed out of our data platform. We were not afraid to build a system from the ground up if we had to.\nWe listed down the following as core capabilities we wanted our data infrastructure to have:\n  Overcome the limitations of batched jobs as listed above.\n  Separate storage and compute as much as possible so they can scale independently.\n  Reduce the number of points of failure.\n  Maintain an audit of changes and have pipelines that can be readily deployed in case of failure.\n  Make it easy to track changes across systems and maintain data lineages to ease troubleshooting.\n  With these considerations in mind, we finally landed on the decision to construct a data platform that builds Domain-Separated Data Lakes using Change Data Capture to replicate source tables.\nData Lake \u0026 CDC Let’s begin by defining the terms ‘data lake’ and ‘data warehouse’. A lot of organizations commit the mistake of using these terms interchangeably, but a data lake is not the same thing as a data warehouse. A data warehouse is a strategic store of information that can readily be used by data analysts and business consumers. A data lake on the other hand is a large repository system intended to store raw data that can be processed and served through the data warehouse when needed.\nFrom an engineering perspective, a data lake needs to replicate and store raw application data from your production databases. This means that any time there’s a change in your production database, the data lake needs to make sure it replicates that change as well. There are various techniques that allow you to identify and expose these changes so that they may be replicated. This method of replicating data by replaying the changes made to the source is known as “Change Data Capture”, or CDC in short.\nChange Data Capture(CDC) can be done in three ways:\n  Query-based\n  Trigger-based\n  Log-based\n  The replication method that we describe at the beginning of this paragraph can be classified as query-based CDC, where we ran scheduled queries to copy data in batches. This method requires an incremental key to maintain markers over tables while polling the data.\nA trigger-based CDC system uses shadow tables and a set of triggers upon monitoring tables to keep track of changes. It is a relatively least used technique among all due to very low performance and high maintenance overhead.\nA log-based CDC system uses database changelogs, e.g. the Write Ahead Logs (WAL) in Postgres, Binlogs in MySQL, Oplogs in MongoDB, etc. to replay the changes on the replicated data store. These systems use several supported plugins to read the logs like Wal2Json, Pgoutput in Postgres, and open-replicator in Mysql. Log-based CDC has a lot of benefits over others like all data changes are captured including DELETES, complete ordering of transactions in which they are applied and the possibility of building real time streams.* Therefore, we used this technique to create the present day replication pipeline at Grofers.*\nReasons for adopting a data lake with incremental processing Building a data lake is costly and time consuming. Before deciding to make such sweeping infrastructural changes, you need to make sure you assess and evaluate it thoroughly. We went through a similar process and this is what we learned we have to gain:\n1. Separation of concerns helps scalability and query efficiency Data lakes being different storage engines than data warehouses, inherently allows us to separate applications’ tables and marts both logically and physically. Therefore, it addresses the issues regarding characteristics and lineages of tables, making analysts aware of them before they do any queries over them. This also becomes one of the checkpoints for the team in case there are any inconsistencies in marts.\nAnother major benefit that a data lake delivers is the separation of storage and compute. The reason behind it is that a data lake is built using file system storages like AWS S3 or traditional HDFS, which costs a lot less when compared with data warehouses like Amazon Redshift. This benefit brings all the more cost value if the duration of queried data is lesser than the overall history of data stored. For example, most of the reports at our organization only query data that goes back one year. However, we have more than 5 years of data stored in our data lake. Had we stored all of this data in our warehouse, we would have had to pay a much greater sum in infrastructure costs.\n2. No more batch polling over the source databases There is no need for batched SQL based data pulls anymore, as we use database logs that give us complete information about all the transactions that take place on the tables. Also, no more risk of losing data because of change marking columns (e.g. created_at, updated_at etc.), since this method does not rely on using any columns to identify changes. We can also replicate DELETE rows over the warehouse, which earlier got missed out due to querying.\n3. Support varied application database engines and business conventions Every database engine has its specific way of handling data types like time, decimal, JSON, and enum columns. Further, we also have many diverse backend teams that design its database architecture keeping the needs of its end-users in mind. As each team follows different conventions depending on their users, their technologies, etc, we often see a large amount of variation in the way similar columns are named. A data lake allows you to homogenise these differences like precision, namings before you present this data to your analysts.\n4. Data screw-ups and failovers Any organization having a large set of databases and systems requires it to have a minimum resolution time for incidents, so that operations can run smoothly. Data lake allows us to do the same, as we do not have to rely on the source database replicas in case any failure occurs. We became more resilient to failures as we can do reloads without affecting the production systems.\nFurthermore, data now goes through multiple stages of processing which enables us to apply integrity and quality checks at the output of each of these stages and raise alerts to catch issues sooner than later.\n5. Real-time analytics and use-cases The data capture mechanism that we have employed generates real-time data streams. These data streams further can be used to serve many use cases like monitoring day-to-day operations, anomaly detection, real-time suggestions, and more machine learning use cases.\n We have around 800 tables (Postgres and MySQL) in different backend systems that get constantly updated. Maintaining this colossal amount of tables only in Redshift (our warehouse) gets our infrastructure cost bundled up. We have a nearly 60:40 ratio in inserts vs updates for a day and such an enormous volume of updates require a lot of data to be reprocessed and updated (higher odds of duplicate data). In a democratized environment, controlling the quality of queries is hard. Thus, poorly written ad-hoc queries querying entire timeline of data started affecting our SLAs because of our inability to scale compute independently. Since data lake allows us to work on partitions and merge rows based on keys, it resolves issues of duplicate rows before loading to the warehouse. Further, CDC streams allow us to capture every single change in tables which resolves the problem of missing updates. Therefore using a log-based CDC along with Data Lake as the architecture for our replication pipelines seemed like the right next step in the evolution of our data platform.\n CDC tools that work with data lake As discussed earlier, the problems in our pipeline were not just with transformation and storage layers. We faced numerous issues around capturing the changes like row-conflicts, locks, missing updates, schema changes. Thus, we had to change the way we get the data from the application databases, and the solution that we found is the use of log-based CDC. There are many cloud-based and paid tools around log-based data capture but since it is the most critical part of architecture we decided to put more weightage on the control of the system. We tried out AWS Data Migration Service that we used traditionally but since it does not fall into the long-term vision of kappa architecture (being a one-to-one pipeline that cannot be reused further), we had to look for other solutions.\nWe wanted fine-grained control over the tool, which would allow us to easily make modifications around monitoring and alerting. Therefore using black-box services was out of the picture.\nWe tried out several open-source projects around CDC like Debezium, Maxwell, SpinalTap, Brooklin. Among all of those, Debezium stood tall in terms of support of database engines (both MySQL and Postgres), snapshotting, column masking, filtering, transformations, and lastly documentation. Further Debezium also has an active Redhat development team that can provide prompt support to us if we can’t get through an issue. We also tried out Confluent source connectors but those being query-based CDC, we decided to not pursue that route further.\nCreating a useful data lake Data lakes being huge repositories of raw and semi-processed data often get their use-case limited just up to the middle layer for processing data and not having any direct value for the business. Also, there is a lot of criticism surrounding the inefficient data lakes which have turned out to be a graveyard dump of files without any active use. Keeping that in mind, we decided not to dump them as general parquet/ORC files, and add some direct business value like active querying by consumers over the same.\nData lakes are a relatively new architectural pattern and there are a lot of open-source projects in the space that add metadata to these lake files which makes them quite similar to the warehouse and can be queried further using Hive, Presto, and many other SQL query engines. Most of these projects are based on the principle that the data lake should be updated in incremental upserts with the use of metadata like bloom filters, partitioning, indexing, etc.\nSome of the prominent ones are as follows:\n  Delta Lake: This project is the most known among the community and being developed by Databricks. It has two implementations, one with Databricks platform and another open-source. Both of them have the same architecture at the base but open-source implementation currently lags on the Databricks Runtime by a mile in terms of compaction, indexing, pruning, and many other features. But it has the most momentum and adoption in end-user tools among all in terms of ACID based data stores.\n  Apache Iceberg: Originally developed by Netflix for storing slow-moving tabular data, it has the most elegant design of them all with schema management (modular OLAP) using manifests. It is relatively lesser known than the other two and lacks a tighter integration with a processing engine like Apache Spark or Flink or a cloud vendor which makes it a little bit difficult to adopt.\n  Apache Hudi: This is the open-source project originally developed by Uber for ingesting and managing storage of large files over DFS (HDFS or cloud storage). It gives a lot of emphasis on performance (like latency and throughput) with deeply-optimized processing implementation like Copy on Write and Merge on Read datasets. It can also be defined in general as the incremental processing of batch data. It’s currently being supported by the AWS ecosystem (via redshift spectrum). This is currently being adopted by us after an extensive POC.\n  Data Lake and CDC in our replication pipeline\nWe used Apache Hudi as the choice of our storage engine for the data lake, primarily because of the performance-driven approach of the same. Most of our tables are created using Copy On Write paradigm as we do not want to serve real-time updates through redshift. After capturing the CDC from source databases using Debezium and Kafka, we put them in S3 so that they can be incrementally consumed by Spark which is the processing engine for Hudi. And since CDC cannot be put in direct format into Hudi tables we have developed an internal tool for the same i.e. Nessie (derived from the lake monster) as it does all the processing over our lake data and puts the same in the warehouse consumption at the final step.\nNessie: the lake monster Nessie is a tool we are developing to provide an abstraction over the processing engine while coupling it with the data lake, as CDC logs need to be processed into an appropriate format so that it can be stored as a Hudi table.\nAn overview of the features to be developed on Nessie is as follows:\n  Handle the intricacies of integrating Apache Hudi along with Debezium CDC like the generation of the incremental key using transaction information from CDC records and schema evolution.\n  Support for different types of raw file dumps (Avro/JSON/parquet) with different dump intervals (minutes/hours/days) having different data types available in MySQL and Postgres.\n  Support records consumption from Kafka topics using either Delta Streamer, Spark Streaming, or its consumers that poll Kafka topics.\n  Support the generation of standard silver tables for further processing.\n  Support monitoring SLAs using markers, compaction, and other table metrics.\n  Things to take care of If you are planning to embark on creating a data lake using Debezium or Hudi, you need to consider certain issues and limitations of these projects. To describe those, let us first go through the basic parameters of Hudi’s working. A Hudi based table works on 3 important parameters of a table i.e.:\n  Record Key (unique id for each row)\n  Incremental Key (maximum unique value state of row version)\n  Partition Key (constant partition value for row)\n  While developing the lake on CDC data, we faced most challenges around the incremental key where we had to generate incremental change value for each change/transaction in Postgres and MySQL DB engines based on their properties like LSN, Binlog Position. We did think of using the modified time in each row for the same, however, it did not work out due to time values precision and parallel transactions.\nWe also had to develop customizations around timestamp handling in Hudi as its Hive integration currently doesn’t support timestamp as a datatype. Finally, tighter integration among warehouse (redshift) and data lake had to be developed to handle schema evolution.\nIn the Debezium, we faced issues around consistency as there were cases of missing data changes. Some of the critical ones are as follows:\n  DBZ-2288: Postgres connector may skip events during snapshot-streaming transition\n  DBZ-2338: LSNs in replication slots are not monotonically increasing\n  Meanwhile, to circumvent it from our side, we decided to extend our validation script to a patching script that tracks and fixes any change event missed. Developing global validation scripts that can handle different database engines having varying versions was also difficult as they ran over both data lake and data warehouse.\nOn the performance side of the overall pipeline, we faced some issues with Kafka regarding the limitation of the topic to a single partition since Debezium follows the strict ordering of messages. We resolved the same via using increased producer request size so that the overall transfer rate of records increases.\nThe Paradigm Shift If you are wondering about the adoption of this pipeline over our organization, we have currently moved nearly all of our critical tables to this along with their consumers. In the future, we are planning to integrate overall data lineage and health monitoring tools also. We are also slowly moving to stream-based kappa architecture as well where we are going to create real-time systems over this. Also, in Hudi we are planning to try out Merge On Read (MOR) for log-based tables in the warehouse. Although a lot of adoption of Hudi’s plans depends on redshift spectrum support for Hudi.\nOverall moving to this architecture has reduced a lot of volatility in our pipelines along with the massive reduction of costs.\nI want to acknowledge the herculean efforts and zeal of the entire Data team at Grofers especiallyApoorva Aggarwal, Ashish Gambhir, Deepu T Philip, Ishank Yadav, Pragun Bhutani, Sangarshanan, Shubham Gupta, Satyam Upadhyay, Satyam Krishna, Sourav Sikka, and the entire team of data analysts for helping this transition. Also, I want to thank open source maintainers of Debezium and Apache Hudi for the development of these awesome projects.\n","wordCount":"3599","inLanguage":"en","image":"https://cdn-images-1.medium.com/max/6000/1*e2wHxBZ0lfZCcp5EXg__Iw.jpeg","datePublished":"2020-09-15T11:30:03Z","dateModified":"2020-09-15T11:30:03Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://akshay2agarwal.github.io/posts/origins_of_data_lake_at_grofers/"},"publisher":{"@type":"Organization","name":"","logo":{"@type":"ImageObject","url":"https://akshay2agarwal.github.io/favicon.ico"}}}</script></head><body class=single id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else{if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}}</script><noscript><style type=text/css>.theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><p class=logo><a href=https://akshay2agarwal.github.io>Akshay Agarwal</a>
<span class=theme-toggle><a id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></a></span></p><ul class=menu id=menu onscroll=menu_on_scroll()></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Origins of Data Lake at Grofers</h1><div class=post-meta>September 15, 2020&nbsp;·&nbsp;17 min&nbsp;·&nbsp;Me</div></header><figure class=entry-cover><img src=https://cdn-images-1.medium.com/max/6000/1*e2wHxBZ0lfZCcp5EXg__Iw.jpeg alt="Designed by [Asif Jamal](https://medium.com/@asifjamal)"><p>Designed by <a href=https://medium.com/@asifjamal>Asif Jamal</a></p></figure><div class=toc><details><summary><div class=details>Table of Contents</div></summary><blockquote><ul><li><a href=#the-beginning>The Beginning</a><ul><li><a href=#1-lack-of-separation-between-raw-and-processed-data><strong>1. Lack of separation between raw and processed data</strong></a></li><li><a href=#2-limitations-of-batched-sql-based-loading>2. Limitations of batched SQL based loading</a></li><li><a href=#3-components-that-would-not-scale>3. Components that would not scale</a></li><li><a href=#4-no-support-for-real-time-pipelines>4. No support for real-time pipelines</a></li></ul></li><li><a href=#data-lake--cdc>Data Lake & CDC</a></li><li><a href=#reasons-for-adopting-a-data-lake-with-incremental-processing>Reasons for adopting a data lake with incremental processing</a><ul><li><a href=#1-separation-of-concerns-helps-scalability-and-query-efficiency>1. Separation of concerns helps scalability and query efficiency</a></li><li><a href=#2-no-more-batch-polling-over-the-source-databases>2. No more batch polling over the source databases</a></li><li><a href=#3-support-varied-application-database-engines-and-business-conventions>3. Support varied application database engines and business conventions</a></li><li><a href=#4-data-screw-ups-and-failovers>4. Data screw-ups and failovers</a></li><li><a href=#5-real-time-analytics-and-use-cases>5. Real-time analytics and use-cases</a></li></ul></li><li><a href=#cdc-tools-that-work-with-data-lake>CDC tools that work with data lake</a></li><li><a href=#creating-a-useful-data-lake>Creating a useful data lake</a></li><li><a href=#nessie-the-lake-monster>Nessie: the lake monster</a></li><li><a href=#things-to-take-care-of>Things to take care of</a></li><li><a href=#the-paradigm-shift>The Paradigm Shift</a></li></ul></blockquote></details></div><div class=post-content><p>Originally published over <a href=https://lambda.grofers.com/origins-of-data-lake-at-grofers-6c011f94b86c>lambda.grofers.com</a></p><h2 id=the-beginning>The Beginning</h2><p>As Data Engineers at the biggest online grocery company in India, one of our major challenges is to democratize data across the organization. But, when we began evaluating how well we were doing on this goal, we realized that frequent errors in our data pipelines had caused our business teams to lose confidence in data. As a result, they were never sure which data source is correct and apt to use for their analyses and had to consult the data platform team at every step. This wasn’t where we aspired to be when we provided the tools for making correct data-based decisions as independently as possible without getting slowed down.</p><p>As modern data platforms gather data from many disparate, disconnected, and diverse systems they are prone to data collection issues like duplicate records, missed updates, etc. To resolve these problems we conducted a thorough study of our data platform and realized that the architectural debts accumulated over time caused most cases of incorrect data. All major functions of our data platform — extraction, transformation, and storage had issues that led to the stated quality concerns with our data platform.</p><p>Let’s first list down <strong>these issues</strong>:</p><h3 id=1-lack-of-separation-between-raw-and-processed-data><strong>1. Lack of separation between raw and processed data</strong></h3><p>When we started our data journey 5 years ago, we did not have the foresight to separate our source tables from our derived tables. Thus the application tables got dumped into the same warehouse along with schema. While this was okay when we only had 20 tables, it became a huge problem when that number grew to cross the 1000 mark.</p><p>Not only were source tables placed right next to the <a href=https://www.sisense.com/glossary/data-mart/>data marts</a> built on top of these tables, oftentimes we would make modifications to the source tables themselves. As a result, data consumers were often unsure about the meaning of data contained in different tables and found it hard to determine which table or column to use as the source of truth.</p><p>From an engineering perspective, it had started becoming difficult for us to trace lineages of data points during troubleshooting, making our MTTR high and causing regular disruption for end users very frequently. This lack of separation also took a toll on our infrastructure bills as we were keeping raw tables along with marts in the same storage.</p><h3 id=2-limitations-of-batched-sql-based-loading>2. Limitations of batched SQL based loading</h3><p>In the beginning, our source tables were replicated using scheduled, batched, SQL based pulls from our production databases.</p><p>These loading jobs had certain problems which were inherent to batched jobs:</p><ol><li><p>These operations needed to rely on one fixed column, e.g. the primary key, “created_at” or “updated_at” fields, which could be used as a marker to keep track of the number of rows that had already been replicated so that the subsequent job could begin where the previous one had left off. However, this failed to replicate the changes that would not be visible through the tracking column. For example, imagine using the “updated_at” column as your tracker and you DELETE a row in your source table.</p></li><li><p>You had to be precise when specifying the boundary conditions like the next marker time from which data has to be queried. Oftentimes, analysts failed to consider this while defining their jobs and you would see duplicate or missing entries around the edges of your conditions.</p></li><li><p>We had some business-critical tables which get a high frequency of updates. So much so that the tables mutate multiple times while the replication is still in progress. This would either cause the jobs to fail because of row conflicts or succeed but produce data that differs from the source since redshift does not have native merge command. To circumvent this problem, we had to add more tools to the platform, which in turn became fragmented, increasing the complexity and adding more points of failure.</p></li></ol><h3 id=3-components-that-would-not-scale>3. Components that would not scale</h3><p>To allow data analysts to program their replication operations without any help from the data engineering team, we introduced a collection of visual drag-drop tools. While this allowed us to scale up our data operations in the beginning, it quickly turned into an unmanageable nightmare. A couple of years down the line, finding the job that populated X table had gone from difficult to impossible.</p><p>Another drawback of some of the tools that we had been using to run the bulk of our jobs had very little to no ability to raise alerts when something went wrong. This led to a situation where we were often unaware of any problems or inconsistencies in our data and only found out when some data analyst raised a concern. Add to that the fact that these tools started taking more time for execution with increasing scale, we had a very expensive problem on our hands.</p><h3 id=4-no-support-for-real-time-pipelines>4. No support for real-time pipelines</h3><p>As the organization grew, we had started seeing more and more real-time use cases come up and there was no way to extend the existing data platform to cater to these scenarios.</p><p>With so many issues plaguing our warehouse, we realized that we had come to the end of the road with the first generation of our data warehouse. It was at this point that we decided to take a step back, think of what we needed out of our data platform. We were not afraid to build a system from the ground up if we had to.</p><p>We listed down the following as <strong>core capabilities</strong> we wanted our data infrastructure to have:</p><ol><li><p>Overcome the limitations of batched jobs as listed above.</p></li><li><p>Separate storage and compute as much as possible so they can scale independently.</p></li><li><p>Reduce the number of points of failure.</p></li><li><p>Maintain an audit of changes and have pipelines that can be readily deployed in case of failure.</p></li><li><p>Make it easy to track changes across systems and maintain data lineages to ease troubleshooting.</p></li></ol><p>With these considerations in mind, we finally landed on the decision to construct a data platform that builds <strong>Domain-Separated Data Lakes</strong> using <strong>Change Data Capture</strong> to replicate source tables.</p><h2 id=data-lake--cdc>Data Lake & CDC</h2><p>Let’s begin by defining the terms ‘data lake’ and ‘data warehouse’. A lot of organizations commit the mistake of using these terms interchangeably, but a data lake is not the same thing as a data warehouse. A data warehouse is a strategic store of information that can readily be used by data analysts and business consumers. A data lake on the other hand is a large repository system intended to store raw data that can be processed and served through the data warehouse when needed.</p><p>From an engineering perspective, a data lake needs to replicate and store raw application data from your production databases. This means that any time there’s a change in your production database, the data lake needs to make sure it replicates that change as well. There are various techniques that allow you to identify and expose these changes so that they may be replicated. This method of replicating data by replaying the changes made to the source is known as “Change Data Capture”, or CDC in short.</p><p>Change Data Capture(CDC) can be done in three ways:</p><ol><li><p>Query-based</p></li><li><p>Trigger-based</p></li><li><p>Log-based</p></li></ol><p>The replication method that we describe at the beginning of this paragraph can be classified as <em>query-based CDC</em>, where we ran scheduled queries to copy data in batches. This method requires an incremental key to maintain markers over tables while polling the data.</p><p>A <em>trigger-based CDC</em> system uses shadow tables and a set of triggers upon monitoring tables to keep track of changes. It is a relatively least used technique among all due to very low performance and high maintenance overhead.</p><p>A <em>log-based CDC</em> system uses database changelogs, e.g. the Write Ahead Logs (WAL) in Postgres, Binlogs in MySQL, Oplogs in MongoDB, etc. to replay the changes on the replicated data store. These systems use several supported plugins to read the logs like Wal2Json, Pgoutput in Postgres, and open-replicator in Mysql. Log-based CDC has a lot of benefits over others like all data changes are captured including DELETES, complete ordering of transactions in which they are applied and the possibility of building real time streams.* Therefore, we used this technique to create the present day replication pipeline at Grofers.*</p><h2 id=reasons-for-adopting-a-data-lake-with-incremental-processing>Reasons for adopting a data lake with incremental processing</h2><p>Building a data lake is costly and time consuming. Before deciding to make such sweeping infrastructural changes, you need to make sure you assess and evaluate it thoroughly. We went through a similar process and this is what we learned we have to gain:</p><h3 id=1-separation-of-concerns-helps-scalability-and-query-efficiency>1. Separation of concerns helps scalability and query efficiency</h3><p>Data lakes being different storage engines than data warehouses, inherently allows us to separate applications’ tables and marts both logically and physically. Therefore, it addresses the issues regarding characteristics and lineages of tables, making analysts aware of them before they do any queries over them. This also becomes one of the checkpoints for the team in case there are any inconsistencies in marts.</p><p>Another major benefit that a data lake delivers is the separation of storage and compute. The reason behind it is that a data lake is built using file system storages like AWS S3 or traditional HDFS, which costs a lot less when compared with data warehouses like Amazon Redshift. This benefit brings all the more cost value if the duration of queried data is lesser than the overall history of data stored. For example, most of the reports at our organization only query data that goes back one year. However, we have more than 5 years of data stored in our data lake. Had we stored all of this data in our warehouse, we would have had to pay a much greater sum in infrastructure costs.</p><h3 id=2-no-more-batch-polling-over-the-source-databases>2. No more batch polling over the source databases</h3><p>There is no need for batched SQL based data pulls anymore, as we use database logs that give us complete information about all the transactions that take place on the tables. Also, no more risk of losing data because of change marking columns (e.g. created_at, updated_at etc.), since this method does not rely on using any columns to identify changes. We can also replicate DELETE rows over the warehouse, which earlier got missed out due to querying.</p><h3 id=3-support-varied-application-database-engines-and-business-conventions>3. Support varied application database engines and business conventions</h3><p>Every database engine has its specific way of handling data types like time, decimal, JSON, and enum columns. Further, we also have many diverse backend teams that design its database architecture keeping the needs of its end-users in mind. As each team follows different conventions depending on their users, their technologies, etc, we often see a large amount of variation in the way similar columns are named. A data lake allows you to homogenise these differences like precision, namings before you present this data to your analysts.</p><h3 id=4-data-screw-ups-and-failovers>4. Data screw-ups and failovers</h3><p>Any organization having a large set of databases and systems requires it to have a minimum resolution time for incidents, so that operations can run smoothly. Data lake allows us to do the same, as we do not have to rely on the source database replicas in case any failure occurs. We became more resilient to failures as we can do reloads without affecting the production systems.</p><p>Furthermore, data now goes through multiple stages of processing which enables us to apply integrity and quality checks at the output of each of these stages and raise alerts to catch issues sooner than later.</p><h3 id=5-real-time-analytics-and-use-cases>5. Real-time analytics and use-cases</h3><p>The data capture mechanism that we have employed generates real-time data streams. These data streams further can be used to serve many use cases like monitoring day-to-day operations, anomaly detection, real-time suggestions, and more machine learning use cases.</p><blockquote><p><em>We have around 800 tables (Postgres and MySQL) in different backend systems that get constantly updated. Maintaining this colossal amount of tables only in Redshift (our warehouse) gets our infrastructure cost bundled up. We have a nearly 60:40 ratio in inserts vs updates for a day and such an enormous volume of updates require a lot of data to be reprocessed and updated (higher odds of duplicate data). In a democratized environment, controlling the quality of queries is hard. Thus, poorly written ad-hoc queries querying entire timeline of data started affecting our SLAs because of our inability to scale compute independently. Since data lake allows us to work on partitions and merge rows based on keys, it resolves issues of duplicate rows before loading to the warehouse. Further, CDC streams allow us to capture every single change in tables which resolves the problem of missing updates. Therefore using a log-based CDC along with Data Lake as the architecture for our replication pipelines seemed like the right next step in the evolution of our data platform.</em></p></blockquote><h2 id=cdc-tools-that-work-with-data-lake>CDC tools that work with data lake</h2><p>As discussed earlier, the problems in our pipeline were not just with transformation and storage layers. We faced numerous issues around capturing the changes like row-conflicts, locks, missing updates, schema changes. Thus, we had to change the way we get the data from the application databases, and the solution that we found is the use of log-based CDC. There are many cloud-based and paid tools around log-based data capture but since it is the most critical part of architecture we decided to put more weightage on the control of the system. We tried out AWS Data Migration Service that we used traditionally but since it does not fall into the long-term vision of kappa architecture (being a one-to-one pipeline that cannot be reused further), we had to look for other solutions.</p><p>We wanted fine-grained control over the tool, which would allow us to easily make modifications around monitoring and alerting. Therefore using black-box services was out of the picture.</p><p>We tried out several open-source projects around CDC like <a href=https://debezium.io/>Debezium</a>, <a href=https://maxwells-daemon.io/>Maxwell</a>, <a href=https://github.com/airbnb/SpinalTap>SpinalTap</a>, <a href=https://github.com/linkedin/brooklin>Brooklin</a>. Among all of those, Debezium stood tall in terms of support of database engines (both MySQL and Postgres), snapshotting, column masking, filtering, transformations, and lastly documentation. Further Debezium also has an active Redhat development team that can provide prompt support to us if we can’t get through an issue. We also tried out Confluent source connectors but those being query-based CDC, we decided to not pursue that route further.</p><h2 id=creating-a-useful-data-lake>Creating a useful data lake</h2><p>Data lakes being huge repositories of raw and semi-processed data often get their use-case limited just up to the middle layer for processing data and not having any direct value for the business. Also, there is a lot of criticism surrounding the inefficient data lakes which have turned out to be a graveyard dump of files without any active use. Keeping that in mind, we decided not to dump them as general parquet/ORC files, and add some direct business value like active querying by consumers over the same.</p><p>Data lakes are a relatively new architectural pattern and there are a lot of open-source projects in the space that add metadata to these lake files which makes them quite similar to the warehouse and can be queried further using Hive, Presto, and many other SQL query engines. Most of these projects are based on the principle that the data lake should be updated in incremental upserts with the use of metadata like bloom filters, partitioning, indexing, etc.</p><p>Some of the prominent ones are as follows:</p><ul><li><p><a href=https://delta.io/>Delta Lake</a>: This project is the most known among the community and being developed by Databricks. It has two implementations, one with Databricks platform and another open-source. Both of them have the same architecture at the base but open-source implementation currently lags on the Databricks Runtime by a mile in terms of compaction, indexing, pruning, and many other features. But it has the most momentum and adoption in end-user tools among all in terms of ACID based data stores.</p></li><li><p><a href=https://iceberg.apache.org/>Apache Iceberg</a>: Originally developed by Netflix for storing slow-moving tabular data, it has the most elegant design of them all with schema management (modular OLAP) using manifests. It is relatively lesser known than the other two and lacks a tighter integration with a processing engine like Apache Spark or Flink or a cloud vendor which makes it a little bit difficult to adopt.</p></li><li><p><a href=https://hudi.apache.org/>Apache Hudi</a>: This is the open-source project originally developed by Uber for ingesting and managing storage of large files over DFS (HDFS or cloud storage). It gives a lot of emphasis on performance (like latency and throughput) with deeply-optimized processing implementation like Copy on Write and Merge on Read datasets. It can also be defined in general as the incremental processing of batch data. It’s currently being supported by the AWS ecosystem (via redshift spectrum). This is currently being adopted by us after an extensive POC.</p></li></ul><p><img src=https://cdn-images-1.medium.com/max/2000/1*xU4lE4l1WHRqvfBMA0MshA.gif alt="Data Lake and CDC in our replication pipeline"><em>Data Lake and CDC in our replication pipeline</em></p><p><em>We used Apache Hudi as the choice of our storage engine for the data lake, primarily because of the <a href=https://databricks.com/session_na20/a-thorough-comparison-of-delta-lake-iceberg-and-hudi>performance-driven approach</a> of the same. Most of our tables are created using Copy On Write paradigm as we do not want to serve real-time updates through redshift. After capturing the CDC from source databases using Debezium and Kafka, we put them in S3 so that they can be incrementally consumed by Spark which is the processing engine for Hudi. And since CDC cannot be put in direct format into Hudi tables we have developed an internal tool for the same i.e. <strong>Nessie</strong> (derived from the lake monster) as it does all the processing over our lake data and puts the same in the warehouse consumption at the final step.</em></p><h2 id=nessie-the-lake-monster>Nessie: the lake monster</h2><p>Nessie is a tool we are developing to provide an abstraction over the processing engine while coupling it with the data lake, as CDC logs need to be processed into an appropriate format so that it can be stored as a Hudi table.</p><p>An overview of the features to be developed on Nessie is as follows:</p><ol><li><p>Handle the intricacies of integrating Apache Hudi along with Debezium CDC like the generation of the incremental key using transaction information from CDC records and schema evolution.</p></li><li><p>Support for different types of raw file dumps (Avro/JSON/parquet) with different dump intervals (minutes/hours/days) having different data types available in MySQL and Postgres.</p></li><li><p>Support records consumption from Kafka topics using either Delta Streamer, Spark Streaming, or its consumers that poll Kafka topics.</p></li><li><p>Support the generation of standard silver tables for further processing.</p></li><li><p>Support monitoring SLAs using markers, compaction, and other table metrics.</p></li></ol><h2 id=things-to-take-care-of>Things to take care of</h2><p>If you are planning to embark on creating a data lake using Debezium or Hudi, you need to consider certain issues and limitations of these projects. To describe those, let us first go through the basic parameters of Hudi’s working. A Hudi based table works on 3 important parameters of a table i.e.:</p><ol><li><p>Record Key (unique id for each row)</p></li><li><p>Incremental Key (maximum unique value state of row version)</p></li><li><p>Partition Key (constant partition value for row)</p></li></ol><p>While developing the lake on CDC data, we faced most challenges around the incremental key where we had to generate incremental change value for each change/transaction in Postgres and MySQL DB engines based on their properties like LSN, Binlog Position. We did think of using the modified time in each row for the same, however, it did not work out due to time values precision and parallel transactions.</p><p>We also had to develop customizations around timestamp handling in Hudi as its Hive integration currently doesn’t support timestamp as a datatype. Finally, tighter integration among warehouse (redshift) and data lake had to be developed to handle schema evolution.</p><p>In the Debezium, we faced issues around consistency as there were cases of missing data changes. Some of the critical ones are as follows:</p><ul><li><p><a href=https://issues.redhat.com/browse/DBZ-2288>DBZ-2288: Postgres connector may skip events during snapshot-streaming transition</a></p></li><li><p><a href=https://issues.redhat.com/browse/DBZ-2338>DBZ-2338: LSNs in replication slots are not monotonically increasing</a></p></li></ul><p>Meanwhile, to circumvent it from our side, we decided to extend our validation script to a patching script that tracks and fixes any change event missed. Developing global validation scripts that can handle different database engines having varying versions was also difficult as they ran over both data lake and data warehouse.</p><p>On the performance side of the overall pipeline, we faced some issues with Kafka regarding the limitation of the topic to a single partition since Debezium follows the strict ordering of messages. We resolved the same via using <a href=https://www.javierholguera.com/2020/06/02/kafka-connect-offset-commit-errors-i/>increased producer request size</a> so that the overall transfer rate of records increases.</p><h2 id=the-paradigm-shift>The Paradigm Shift</h2><p>If you are wondering about the adoption of this pipeline over our organization, we have currently moved nearly all of our critical tables to this along with their consumers. In the future, we are planning to integrate overall data lineage and health monitoring tools also. We are also slowly moving to <a href=https://milinda.pathirage.org/kappa-architecture.com/>stream-based kappa architecture</a> as well where we are going to create real-time systems over this. Also, in Hudi we are planning to try out Merge On Read (MOR) for log-based tables in the warehouse. Although a lot of adoption of Hudi’s plans depends on <a href=https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-redshift-spectrum-adds-support-for-querying-open-source-apache-hudi-and-delta-lake/>redshift spectrum support for Hudi</a>.</p><p>Overall moving to this architecture has reduced a lot of volatility in our pipelines along with the massive reduction of costs.</p><p><em>I want to acknowledge the herculean efforts and zeal of the entire Data team at Grofers especially<a href=http://medium.com/@apoorva.ag29> Apoorva Aggarwal</a>, <a href=https://medium.com/@ashishgambhir>Ashish Gambhir</a>, <a href=https://github.com/deepu-tp>Deepu T Philip</a>, <a href=https://www.linkedin.com/in/ishankydv/>Ishank Yadav</a>, <a href=https://www.linkedin.com/in/pragunbhutani>Pragun Bhutani</a>, <a href=https://github.com/sangarshanan>Sangarshanan</a>, <a href=https://www.linkedin.com/in/shubhamg931/>Shubham Gupta</a>, <a href=https://in.linkedin.com/in/satyam-upadhyay-950a5881>Satyam Upadhyay</a>, <a href=https://www.linkedin.com/in/satyamkrishna92/>Satyam Krishna</a>, <a href=https://www.linkedin.com/in/sourav-sikka-36339820/>Sourav Sikka</a>, and the entire team of data analysts for helping this transition. Also, I want to thank open source maintainers of <a href=https://debezium.io/>Debezium</a> and <a href=https://hudi.apache.org/>Apache Hudi</a> for the development of these awesome projects.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://akshay2agarwal.github.io/tags/data-lake>data lake</a></li><li><a href=https://akshay2agarwal.github.io/tags/apache-hudi>apache hudi</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Origins of Data Lake at Grofers on twitter" href="https://twitter.com/intent/tweet/?text=Origins%20of%20Data%20Lake%20at%20Grofers&url=https%3a%2f%2fakshay2agarwal.github.io%2fposts%2forigins_of_data_lake_at_grofers%2f&hashtags=datalake%2capachehudi"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-253.927 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Origins of Data Lake at Grofers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fakshay2agarwal.github.io%2fposts%2forigins_of_data_lake_at_grofers%2f&title=Origins%20of%20Data%20Lake%20at%20Grofers&summary=Origins%20of%20Data%20Lake%20at%20Grofers&source=https%3a%2f%2fakshay2agarwal.github.io%2fposts%2forigins_of_data_lake_at_grofers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0v-129.439c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02v-126.056c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768h75.024zm-307.552-334.556c-25.674.0-42.448 16.879-42.448 39.002.0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Origins of Data Lake at Grofers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fakshay2agarwal.github.io%2fposts%2forigins_of_data_lake_at_grofers%2f&title=Origins%20of%20Data%20Lake%20at%20Grofers"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zm-119.474 108.193c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zm-160.386-29.702c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Origins of Data Lake at Grofers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fakshay2agarwal.github.io%2fposts%2forigins_of_data_lake_at_grofers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978v-192.915h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Origins of Data Lake at Grofers on whatsapp" href="https://api.whatsapp.com/send?text=Origins%20of%20Data%20Lake%20at%20Grofers%20-%20https%3a%2f%2fakshay2agarwal.github.io%2fposts%2forigins_of_data_lake_at_grofers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23-13.314-11.876-22.304-26.542-24.916-31.026s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Origins of Data Lake at Grofers on telegram" href="https://telegram.me/share/url?text=Origins%20of%20Data%20Lake%20at%20Grofers&url=https%3a%2f%2fakshay2agarwal.github.io%2fposts%2forigins_of_data_lake_at_grofers%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47A3.38 3.38.0 0126.49 29.86zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>© Akshay Agarwal 2020</span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo️️</a>️</span>
<span>&#183;</span>
<span>Theme️ <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top"><button class=top-link id=top-link type=button><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6"><path d="M12 6H0l6-6z"/></svg></button></a>
<script src=https://akshay2agarwal.github.io/assets/js/highlight.min.e7afc2928c0925d65c4732dfebe147014d91299a98e819e4b42f25c4fa68e91c.js integrity="sha256-56/CkowJJdZcRzLf6+FHAU2RKZqY6BnktC8lxPpo6Rw="></script><script>hljs.initHighlightingOnLoad();</script><script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();document.querySelector(this.getAttribute("href")).scrollIntoView({behavior:"smooth"});});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script></body></html>